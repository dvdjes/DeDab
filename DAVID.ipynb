{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "import random as random\n",
    "from collections import Counter\n",
    "import string\n",
    "from random import randint, random, sample\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary functions\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2)\n",
    "    return float(len(s1.intersection(s2)) / len(s1.union(s2)))\n",
    "\n",
    "def EucDist(vector1, vector2):\n",
    "    a = list(vector1)\n",
    "    b = list(vector2)\n",
    "    \n",
    "    c = [a_i - b_i for a_i, b_i in zip(a, b)]\n",
    "    dist = np.linalg.norm(c)\n",
    "    \n",
    "    return dist\n",
    "\n",
    "def makeDataSet(data):\n",
    "    models = list(data.keys())\n",
    "    N = len(models)\n",
    "    i = 0\n",
    "    data_list = []\n",
    "\n",
    "    while i < N:\n",
    "        if len(data[models[i]]) == 1:\n",
    "            data_list.append(data[models[i]][0])\n",
    "\n",
    "        else:\n",
    "            for duplicate in data[models[i]]:\n",
    "                data_list.append(duplicate)\n",
    "\n",
    "        i+=1\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "def weightsED(bvm, pairs):\n",
    "\n",
    "    diff_cols = []\n",
    "\n",
    "    for pair in pairs:\n",
    "        v0 = bvm.iloc[pair[0]]  \n",
    "        v1 = bvm.iloc[pair[1]]  \n",
    "        v2 = v1 - v0\n",
    "        l = [i for i,k in enumerate(v2) if k != 0]\n",
    "        diff_cols.extend(l)\n",
    "\n",
    "    x = Counter(diff_cols)\n",
    "    x = x.most_common()\n",
    "\n",
    "    OldMax = x[0][1]\n",
    "    OldMin = x[-1][1]\n",
    "    NewMax = 2\n",
    "    NewMin = 0\n",
    "\n",
    "    new_weights = []\n",
    "    for i in range(len(bvm.columns)):\n",
    "        if i in diff_cols:\n",
    "            OldValue = [k for k in x if k[0] == i][0][1]\n",
    "            NewValue = (((OldValue - OldMin) * (NewMax - NewMin)) / (OldMax - OldMin)) + NewMin   \n",
    "            new_weights.append(NewValue)\n",
    "        else:\n",
    "            new_weights.append(1)\n",
    "    \n",
    "    return new_weights\n",
    "\n",
    "def calculateTruePairs(data_list):\n",
    "    pairs = []\n",
    "\n",
    "    for i in range(len(data_list)):\n",
    "        for j in range(len(data_list)):\n",
    "            if (data_list[i]['modelID'] == data_list[j]['modelID']) & (i != j):\n",
    "                if (j,i) not in pairs:\n",
    "                    pairs.append((i,j))\n",
    "                else:\n",
    "                    None\n",
    "    return pairs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "# Opening JSON file\n",
    "f = open('data.json')\n",
    " \n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "# More workable data\n",
    "data_list = makeDataSet(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Creating binary vector matrix\n",
    "\n",
    "def binaryVectorMatrix(data, threshold):\n",
    "    # Step 1.1: List of words from titles    \n",
    "    models = list(data.keys())\n",
    "    N = len(models)\n",
    "    i = 0\n",
    "    words = []\n",
    "\n",
    "    while i < N:\n",
    "        if len(data[models[i]]) == 1:\n",
    "            words.extend(data[models[i]][0]['title'].split())\n",
    "        else:\n",
    "            for duplicate in data[models[i]]:\n",
    "                words.extend(duplicate['title'].split())\n",
    "        i+=1\n",
    "\n",
    "    #Step 1.1: Clean list of words\n",
    "    words = [w.lower() for w in words]\n",
    "    words = [w.replace('\"', '-inch') for w in words]\n",
    "    for char in string.punctuation:\n",
    "        words = [w.strip(char) for w in words]\n",
    "\n",
    "    x = Counter(words)\n",
    "    x = x.most_common()\n",
    "    filtered_x = []\n",
    "\n",
    "    for tup in x:\n",
    "        if tup[1] >= threshold:\n",
    "            filtered_x.append(tup[0])\n",
    "\n",
    "    filtered_x.remove(\"\")\n",
    "\n",
    "    #Step 1.2: Clean titles\n",
    "    models = list(data.keys())\n",
    "    N = len(models)\n",
    "    i = 0\n",
    "    titles = []\n",
    "    shops = []\n",
    "\n",
    "    while i < N:\n",
    "        if len(data[models[i]]) == 1:\n",
    "            title = data[models[i]][0]['title'].lower()\n",
    "            title = title.replace('\"', '-inch')\n",
    "            titles.append(title)\n",
    "\n",
    "        else:\n",
    "            for duplicate in data[models[i]]:\n",
    "                title = duplicate['title'].lower()\n",
    "                title = title.replace('\"', '-inch')\n",
    "                titles.append(title)\n",
    "\n",
    "        i+=1\n",
    "    \n",
    "    #Step 1.3: Fill dataframe\n",
    "    df = pd.DataFrame(columns=filtered_x)\n",
    "    \n",
    "    new_row = []\n",
    "    for title in titles:\n",
    "        for feature in filtered_x:\n",
    "            if feature in title:\n",
    "                new_row.append(1)\n",
    "            else:\n",
    "                new_row.append(0)\n",
    "\n",
    "        df.loc[len(df)] = new_row\n",
    "        new_row = []\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create signature matrix with Minhashing\n",
    "# find first 1 in row\n",
    "\n",
    "def test_minHash(data, N):\n",
    "    dft = data.transpose()\n",
    "    signmatrix = np.full((N, len(dft.columns)), np.inf)\n",
    "    hash_values = []\n",
    "\n",
    "    for i in range(N):\n",
    "        dft = shuffle(dft)      \n",
    "        \n",
    "        for product in dft:\n",
    "            value = list(dft[product]).index(1)\n",
    "            signmatrix[i][product] = value\n",
    "        \n",
    "    return signmatrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1: Create matrix with all bucket assignments per band for each observation\n",
    "def test_LSH(M, b, r):\n",
    "    \n",
    "    n, d = M.shape\n",
    "    assert(d==b*r)\n",
    "\n",
    "    bucketmatrix = np.full((b, n), 0)    \n",
    "\n",
    "    k=0\n",
    "    for band in range(b):\n",
    "        signature_list = []\n",
    "\n",
    "        for product in range(n):\n",
    "            partial_signature = M[product, k:r+k]\n",
    "            mod_par_sig = list(partial_signature % 6)\n",
    "            \n",
    "            if mod_par_sig not in signature_list:\n",
    "                signature_list.append(mod_par_sig)\n",
    "            else:\n",
    "                None\n",
    "\n",
    "            bucket = signature_list.index(mod_par_sig)\n",
    "            bucketmatrix[band][product] = bucket    \n",
    "        \n",
    "        k = k + r\n",
    "        \n",
    "    return bucketmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.2: Create candidate pairs with the bucket matrix\n",
    "def test_candidatePairs(bm, b, r):\n",
    "    candidate_pairs = set() \n",
    "    hashbuckets = collections.defaultdict(set)\n",
    "\n",
    "    for band_id in range(b):\n",
    "        band = list(bm[band_id])\n",
    "        for bucket in set(band):\n",
    "            for index in range(len(band)):\n",
    "                if band[index] == bucket:\n",
    "                    hashbuckets[bucket].add(index)\n",
    "                else:\n",
    "                    None\n",
    "\n",
    "        candidate_pairs = set() \n",
    "        for bucket in hashbuckets.values():\n",
    "            if len(bucket) > 1:\n",
    "                for pair in itertools.permutations(bucket, 2):\n",
    "                    candidate_pairs.add(pair)\n",
    "\n",
    "    return candidate_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create candidate pairs with LSH\n",
    "\n",
    "# Step 3.1: Create matrix with all bucket assignments per band for each observation\n",
    "def LSH(M, b, r):\n",
    "    \n",
    "    n, d = M.shape\n",
    "    assert(d==b*r)\n",
    "\n",
    "    bucketmatrix = np.full((b, n), 0)    \n",
    "\n",
    "    k=0\n",
    "    for band in range(b):\n",
    "        signature_list = []\n",
    "\n",
    "        for product in range(n):\n",
    "            partial_signature = list(M[product, k:r+k])\n",
    "\n",
    "            if partial_signature not in signature_list:\n",
    "                signature_list.append(partial_signature)\n",
    "            else:\n",
    "                None\n",
    "\n",
    "            bucket = signature_list.index(partial_signature)\n",
    "            bucketmatrix[band][product] = bucket    \n",
    "        \n",
    "        k = k + r\n",
    "        \n",
    "    return bucketmatrix\n",
    "\n",
    "# Step 3.2: Create candidate pairs with the bucket matrix\n",
    "def candidatePairs(bm, b, r):\n",
    "    candidate_pairs = set() \n",
    "    hashbuckets = collections.defaultdict(set)\n",
    "\n",
    "    for band_id in range(b):\n",
    "        band = list(bm[band_id])\n",
    "        for bucket in set(band):\n",
    "            for index in range(len(band)):\n",
    "                if band[index] == bucket:\n",
    "                    hashbuckets[bucket].add(index)\n",
    "                else:\n",
    "                    None\n",
    "\n",
    "        candidate_pairs = set() \n",
    "        for bucket in hashbuckets.values():\n",
    "            if len(bucket) > 1:\n",
    "                for pair in itertools.combinations(bucket, 2):\n",
    "                    candidate_pairs.add(pair)\n",
    "\n",
    "    return candidate_pairs\n",
    "\n",
    "# Step 3.3: Create set of LSH-pairs that satisfy threshold\n",
    "def LSHpairs(cp, sm, b, r, jac_threshold):\n",
    "    \n",
    "    lsh_pairs = set()\n",
    "    for (i, j) in cp:\n",
    "        if jaccard_similarity(sm[i], sm[j]) > jac_threshold:\n",
    "            lsh_pairs.add((i, j))\n",
    "            \n",
    "    return lsh_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Calculate similarity measure based on Euclidean distance and predict duplicates\n",
    "def predDups(bvm, obs, threshold, lshp, weights, weighted=False):\n",
    "    shop = data_list[obs]['shop']\n",
    "    \n",
    "    # Search within LSH-pairs\n",
    "    pairs = []\n",
    "    \n",
    "    for val in lshp:\n",
    "        if val[0] == obs:\n",
    "            # Filter products from the same webshop\n",
    "            if data_list[val[1]]['shop'] != shop:\n",
    "                pairs.append(val[1])\n",
    "            else:\n",
    "                None\n",
    "        else:\n",
    "            None\n",
    "        \n",
    "    nb_dist = []    \n",
    "    for pair in pairs: \n",
    "        if weighted == True:\n",
    "            dist = EucDist(weights[obs] * bvm.iloc[obs], weights[pair] * bvm.iloc[pair])\n",
    "        else:\n",
    "            dist = EucDist(bvm.iloc[obs], bvm.iloc[pair])\n",
    "            \n",
    "        if dist < threshold:\n",
    "                nb_dist.append(pair) \n",
    "    \n",
    "    return nb_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize\n",
      "Ready for blast-off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1624/1624 [00:05<00:00, 283.42it/s]\n"
     ]
    }
   ],
   "source": [
    "#params\n",
    "min_occurences = 1\n",
    "b = 14 #10\n",
    "r = 6 #6\n",
    "hash_fn = r * b#50\n",
    "jac_threshold = 0.68\n",
    "#jac_threshold = (1/b)**(1/r)\n",
    "dist_threshold = 6\n",
    "print('Initialize')\n",
    "\n",
    "\n",
    "bvm = binaryVectorMatrix(data, min_occurences)\n",
    "# Do functions\n",
    "data_list = makeDataSet(data)\n",
    "pairs = calculateTruePairs(data_list)\n",
    "weights = weightsED(bvm, pairs)\n",
    "\n",
    "\n",
    "sm = test_minHash(bvm, hash_fn)\n",
    "bm = test_LSH(sm, b, r)\n",
    "cp = test_candidatePairs(bm, b, r)\n",
    "lshp = LSHpairs(cp, sm, b, r, jac_threshold)\n",
    "\n",
    "# Check predictions\n",
    "preds = pd.DataFrame(columns=['product','predicted_duplicates','true_duplicates'])\n",
    "\n",
    "print('Ready for blast-off')\n",
    "for product in tqdm(range(len(data_list))):\n",
    "    true_duplicates = []\n",
    "    predicted_duplicates = predDups(bvm, product, dist_threshold, lshp, weights)\n",
    "    \n",
    "    for item in range(len(data_list)):\n",
    "        if (data_list[item]['modelID'] == data_list[product]['modelID']) & (item!=product):\n",
    "            true_duplicates.append(item)\n",
    "    \n",
    "    preds = preds.append({'product':product, 'predicted_duplicates':predicted_duplicates, 'true_duplicates':true_duplicates}, \n",
    "                 ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798 842 142 0.16864608076009502\n"
     ]
    }
   ],
   "source": [
    "total_predictions = 0\n",
    "correct_predictions = 0\n",
    "total_dups = 0\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    for prediction in preds.iloc[i]['predicted_duplicates']:     \n",
    "        total_predictions+=1\n",
    "        if prediction in preds.iloc[i]['true_duplicates']:\n",
    "            correct_predictions +=1\n",
    "    for duplicate in preds.iloc[i]['true_duplicates']: \n",
    "        total_dups+=1\n",
    "            \n",
    "print(total_dups, total_predictions, correct_predictions, correct_predictions / total_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(pairs, data_list, bvm, dist_threshold, lshp, bootstraps):\n",
    "    \n",
    "    duplicates = set()\n",
    "    for pair in pairs:\n",
    "        duplicates.add(pair[0])\n",
    "        duplicates.add(pair[1])\n",
    "\n",
    "    total_duplicates = len(duplicates)\n",
    "\n",
    "    # Set up bootstrap\n",
    "    for i in tqdm(range(bootstraps)):\n",
    "\n",
    "        total = range(0,1624)\n",
    "        train = sample(range(0, 1623), int(1624*0.63))\n",
    "        test = [x for x in total if x not in randomlist]\n",
    "\n",
    "        train_datalist = []\n",
    "        for num in train:\n",
    "            train_datalist.append(data_list[num])\n",
    "\n",
    "        test_datalist = []\n",
    "        for num in test:\n",
    "            test_datalist.append(data_list[num])\n",
    "\n",
    "        train_pairs = calculateTruePairs(train_datalist)\n",
    "        weights = weightsED(bvm, train_pairs)\n",
    "\n",
    "        # Make predictions\n",
    "        total_predictions = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for product in test:\n",
    "            predicted_duplicates = predDups(bvm, product, dist_threshold, lshp, weights, weighted=True)\n",
    "            total_predictions = total_predictions + len(predicted_duplicates)\n",
    "\n",
    "            for pd in predicted_duplicates:\n",
    "                if ((pd, product) in pairs) | ((product, pd) in pairs):\n",
    "                    correct_predictions = correct_predictions + 1\n",
    "        \n",
    "        print(total_duplicates, total_predictions, correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Evaluation\n",
    "\n",
    "# Step 5.1: Evaluation of whole algorithm\n",
    "\n",
    "# Set parameters\n",
    "min_occurences = 1\n",
    "b = 20 \n",
    "r = 4 \n",
    "hash_fn = r * b\n",
    "jac_threshold = 0.75\n",
    "#jac_threshold = (1/b)**(1/r)\n",
    "dist_threshold = 6\n",
    "print('Initialize')\n",
    "bvm = binaryVectorMatrix(data, min_occurences)\n",
    "# Do functions\n",
    "\n",
    "data_list = makeDataSet(data)\n",
    "\n",
    "sm = test_minHash(bvm, hash_fn)\n",
    "bm = test_LSH(sm, b, r)\n",
    "cp = test_candidatePairs(bm, b, r)\n",
    "lshp = LSHpairs(cp, sm, b, r, jac_threshold)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:02,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691 16 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [00:00<00:01,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691 14 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [00:01<00:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691 21 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [00:01<00:00,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691 17 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691 20 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#randomlist = sample(range(0, len(bvm.columns)-1), int(len(bvm.columns))*0.63)\n",
    "randomlist = sample(range(0, 1623), int(1624*0.63))\n",
    "bootstrap(pairs, data_list, bvm, dist_threshold, lshp, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = set()\n",
    "for pair in pairs:\n",
    "    duplicates.add(pair[0])\n",
    "    duplicates.add(pair[1])\n",
    "\n",
    "total_duplicates = len(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PC(cp, sm, b, r):\n",
    "    for t in range(1,11,1):\n",
    "        t = t/10\n",
    "        lshp = LSHpairs(cp, sm, b, r, t)\n",
    "        correct_pairs = 0\n",
    "        for pair in lshp:\n",
    "            if pair in pairs:\n",
    "                correct_pairs = correct_pairs + 1\n",
    "                \n",
    "    return correct_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCandPQ(cp, sm, b, r, pairs):\n",
    "    l_cp = []\n",
    "    l_comp_made = []\n",
    "    \n",
    "    for t in range(1,11,1):\n",
    "        print(t)\n",
    "        t = t/10\n",
    "        lshp = LSHpairs(cp, sm, b, r, t)\n",
    "        l_comp_made.append(len(lshp))\n",
    "        correct_pairs = 0\n",
    "        for pair in lshp:\n",
    "            if pair in pairs:\n",
    "                correct_pairs = correct_pairs + 1\n",
    "                \n",
    "        l_cp.append(correct_pairs)\n",
    "    return l_cp, l_comp_made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateTruePairs(data_list):\n",
    "    pairs = []\n",
    "\n",
    "    for i in range(len(data_list)):\n",
    "        for j in range(len(data_list)):\n",
    "            if (data_list[i]['modelID'] == data_list[j]['modelID']) & (i != j):\n",
    "                if (j,i) not in pairs:\n",
    "                    pairs.append((i,j))\n",
    "                else:\n",
    "                    None\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = calculateTruePairs(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "a,b = PCandPQ(cp, sm, 14, 6, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[334, 334, 331, 324, 285, 158, 70, 16, 4, 0]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1151386, 1145112, 1099276, 755624, 236380, 46592, 8540, 1810, 314, 0]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITITIAL IDEA, DIDNT WORK WITH BOOTSTRAPPING\n",
    "#The idea was to make a dataset that would randomly change some of the weights. if this had a positive effect on the data this new weight would be saved. otherwise they would be reset.\n",
    "# Set parameters\n",
    "\"\"\"\n",
    "\n",
    "min_occurences = 1\n",
    "b = 14 #10\n",
    "r = 6 #6\n",
    "hash_fn = r * b#50\n",
    "jac_threshold = 0.7\n",
    "#jac_threshold = (1/b)**(1/r)\n",
    "dist_threshold = 6\n",
    "print('Parameters set')\n",
    "\n",
    "# Do functions\n",
    "bvm = binaryVectorMatrix(data, min_occurences)\n",
    "sm = test_minHash(bvm, hash_fn)\n",
    "bm = test_LSH(sm, b, r)\n",
    "cp = test_candidatePairs(bm, b, r)\n",
    "print(len(cp))\n",
    "lshp = LSHpairs(cp, sm, b, r, jac_threshold)\n",
    "print(len(lshp))\n",
    "print('Functions completed')\n",
    "\n",
    "# Check predictions\n",
    "preds = pd.DataFrame(columns=['product','predicted_duplicates','true_duplicates'])\n",
    "\n",
    "print('Ready for blast-off')\n",
    "for product in tqdm(range(len(data_list))):\n",
    "    true_duplicates = []\n",
    "    predicted_duplicates = predDups(bvm, product, dist_threshold, lshp)\n",
    "    \n",
    "    for item in range(len(data_list)):\n",
    "        if (data_list[item]['modelID'] == data_list[product]['modelID']) & (item!=product):\n",
    "            true_duplicates.append(item)\n",
    "    \n",
    "    preds = preds.append({'product':product, 'predicted_duplicates':predicted_duplicates, 'true_duplicates':true_duplicates}, \n",
    "                 ignore_index = True)\n",
    "    \n",
    "    \n",
    "total_predictions = 0\n",
    "correct_predictions = 0\n",
    "total_dups = 0\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    for prediction in preds.iloc[i]['predicted_duplicates']:     \n",
    "        total_predictions+=1\n",
    "        if prediction in preds.iloc[i]['true_duplicates']:\n",
    "            correct_predictions +=1\n",
    "    for duplicate in preds.iloc[i]['true_duplicates']: \n",
    "        total_dups+=1\n",
    "            \n",
    "print(total_dups, total_predictions, correct_predictions, correct_predictions / total_predictions)\n",
    "total_predictionsb = total_predictions\n",
    "correct_predictionsb = correct_predictions\n",
    "total_dupsb = total_dups\n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    \n",
    "    # Check predictions\n",
    "    rfs = np.ones(len(bvm.columns),dtype= int)\n",
    "    for j in range (0, 50):\n",
    "        rfs[random.randint(0, len(bvm.columns)-1)]= 3\n",
    "        \n",
    "    bvm2 = rfs*bvm\n",
    "\n",
    "    preds = pd.DataFrame(columns=['product','predicted_duplicates','true_duplicates'])\n",
    "\n",
    "    print('Ready for blast-off2')\n",
    "    for product in tqdm(range(len(data_list))):\n",
    "        true_duplicates = []\n",
    "        predicted_duplicates = predDups(bvm2, product, dist_threshold, lshp)\n",
    "\n",
    "        for item in range(len(data_list)):\n",
    "            if (data_list[item]['modelID'] == data_list[product]['modelID']) & (item!=product):\n",
    "                true_duplicates.append(item)\n",
    "\n",
    "        preds = preds.append({'product':product, 'predicted_duplicates':predicted_duplicates, 'true_duplicates':true_duplicates}, \n",
    "                     ignore_index = True)\n",
    "\n",
    "    total_predictions = 0\n",
    "    correct_predictions = 0\n",
    "    total_dups = 0\n",
    "\n",
    "    for i in range(len(preds)):\n",
    "        for prediction in preds.iloc[i]['predicted_duplicates']:     \n",
    "            total_predictions+=1\n",
    "            if prediction in preds.iloc[i]['true_duplicates']:\n",
    "                correct_predictions +=1\n",
    "        for duplicate in preds.iloc[i]['true_duplicates']: \n",
    "            total_dups+=1\n",
    "\n",
    "    print(total_dups, total_predictions, correct_predictions, correct_predictions / total_predictions)\n",
    "    print(correct_predictions / total_predictions)\n",
    "    if (correct_predictionsb / total_predictionsb)<(correct_predictions / total_predictions):\n",
    "        bvm=bvm2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for blast-off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1624/1624 [04:05<00:00,  6.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# Check predictions\n",
    "preds2 = pd.DataFrame(columns=['product','predicted_duplicates','true_duplicates'])\n",
    "\n",
    "print('Ready for blast-off')\n",
    "for product in tqdm(range(len(data_list))):\n",
    "    true_duplicates = []\n",
    "    predicted_duplicates = predDups(bvm, product, 3.5, lshp, weights, weighted=True)\n",
    "    \n",
    "    for item in range(len(data_list)):\n",
    "        if (data_list[item]['modelID'] == data_list[product]['modelID']) & (item!=product):\n",
    "            true_duplicates.append(item)\n",
    "    \n",
    "    preds2 = preds2.append({'product':product, 'predicted_duplicates':predicted_duplicates, 'true_duplicates':true_duplicates}, \n",
    "                 ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create signature matrix with Minhashing\n",
    "# approximate permutations\n",
    "\"\"\"''\n",
    "def minHash(data, N):\n",
    "    dft = data.transpose()\n",
    "\n",
    "    signmatrix = np.full((len(dft.columns), N), np.inf)\n",
    "    hash_values = []\n",
    "\n",
    "    np.random.seed(1)\n",
    "    for row in range(len(dft)):\n",
    "        hash_row = []\n",
    "        for i in range(N):\n",
    "            hash_value = (randint(0, N) + randint(1, N) * (row+1)) % 1063\n",
    "            hash_row.append(hash_value)\n",
    "        hash_values.append(hash_row)\n",
    "        for column in dft:\n",
    "            if (dft.iloc[row][column] == 1):\n",
    "                for i in range(len(hash_values[row])):\n",
    "                    value = hash_values[row][i]\n",
    "                    if value < signmatrix[column][i]:\n",
    "                        signmatrix[column][i] = value\n",
    "        \n",
    "    return signmatrix\n",
    "\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
